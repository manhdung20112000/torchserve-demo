2022-06-22T11:36:24,986 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:36:24,986 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:36:25,059 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: net=mnist_net.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:36:25,059 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: net=mnist_net.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:36:25,066 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:36:25,066 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:36:25,076 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist_net.mar
2022-06-22T11:36:25,076 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist_net.mar
2022-06-22T11:36:25,077 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnist_net.mar
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnist_net.mar
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T11:36:25,077 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnist_net.mar
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnist_net.mar
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T11:36:25,083 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:36:25,083 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:36:25,112 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:36:25,112 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:36:25,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:36:25,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:36:25,232 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-22T11:36:25,232 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-22T11:37:25,461 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,461 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03841018676758|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,461 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63873672485352|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,462 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,462 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.2987551867219915|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,462 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:277|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,462 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:4|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,463 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10506.5546875|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,463 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4840.34375|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,463 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:33.7|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:39,472 [INFO ] pool-2-thread-1 ACCESS_LOG - /127.0.0.1:59516 "GET /ping HTTP/1.1" 200 5
2022-06-22T11:37:39,473 [INFO ] pool-2-thread-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03882598876953|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63832092285156|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.680497925311203|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:323|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,424 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:13|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,424 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10464.32421875|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,424 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4857.859375|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,424 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:34.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.02838516235352|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.64876174926758|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.672199170124481|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:322|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:17|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10421.7109375|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,427 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4916.0859375|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,427 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:34.2|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.0410499572754|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.6360969543457|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.9377593360995853|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:354|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,437 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:41|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,437 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10652.80078125|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,437 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4685.05078125|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,437 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:32.8|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:59,570 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:59518 "POST /predictions/net HTTP/1.1" 404 3
2022-06-22T11:40:59,571 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:05,702 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /127.0.0.1:59520 "GET /predictions/net HTTP/1.1" 404 1
2022-06-22T11:41:05,703 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:05,774 [INFO ] epollEventLoopGroup-3-4 ACCESS_LOG - /127.0.0.1:59522 "GET /favicon.ico HTTP/1.1" 404 0
2022-06-22T11:41:05,775 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:08,282 [INFO ] epollEventLoopGroup-3-5 ACCESS_LOG - /127.0.0.1:59524 "GET /predictions/ HTTP/1.1" 404 0
2022-06-22T11:41:08,282 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:15,491 [INFO ] epollEventLoopGroup-3-6 ACCESS_LOG - /127.0.0.1:59526 "GET / HTTP/1.1" 405 0
2022-06-22T11:41:15,491 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:19,223 [INFO ] epollEventLoopGroup-3-7 ACCESS_LOG - /127.0.0.1:59528 "GET / HTTP/1.1" 405 0
2022-06-22T11:41:19,223 [INFO ] epollEventLoopGroup-3-7 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03789520263672|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63925170898438|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.854771784232365|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:344|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:13|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10694.8515625|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4646.97265625|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:32.5|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03767013549805|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63947677612305|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.20746887966805|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:266|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10756.32421875|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,401 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4602.94140625|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,401 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:32.1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:26,370 [INFO ] epollEventLoopGroup-3-9 ACCESS_LOG - /127.0.0.1:59532 "POST /predictions/mnist-net HTTP/1.1" 404 1
2022-06-22T11:42:26,372 [INFO ] epollEventLoopGroup-3-9 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:43:25,396 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,396 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03764724731445|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63949966430664|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.199170124481328|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:265|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:2|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10759.18359375|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4606.359375|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:32.1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:30,127 [INFO ] epollEventLoopGroup-3-10 ACCESS_LOG - /127.0.0.1:59534 "POST /predictions/net HTTP/1.1" 404 1
2022-06-22T11:43:30,127 [INFO ] epollEventLoopGroup-3-10 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:43:40,427 [INFO ] epollEventLoopGroup-3-11 ACCESS_LOG - /127.0.0.1:59536 "POST /predictions/mnist_net HTTP/1.1" 404 0
2022-06-22T11:43:40,428 [INFO ] epollEventLoopGroup-3-11 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:43:45,182 [INFO ] epollEventLoopGroup-3-12 ACCESS_LOG - /127.0.0.1:59538 "POST /predictions/mnist-net HTTP/1.1" 404 1
2022-06-22T11:43:45,182 [INFO ] epollEventLoopGroup-3-12 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:44:41,353 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:44:41,353 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:44:41,431 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: net=mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:44:41,431 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: net=mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:44:41,440 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:44:41,440 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:44:41,452 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:44:41,452 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model net
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model net
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model net
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model net
2022-06-22T11:44:41,464 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model net loaded.
2022-06-22T11:44:41,464 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model net loaded.
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: net, count: 1
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: net, count: 1
2022-06-22T11:44:41,468 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:41,468 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:41,469 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:44:41,469 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:44:41,498 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:44:41,498 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:44:41,585 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:44:41,585 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:44:41,790 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,790 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03741836547852|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,790 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63972854614258|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.12448132780083|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:256|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:16|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10802.578125|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4562.96484375|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,792 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:31.8|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,794 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:41,794 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27197
2022-06-22T11:44:41,795 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:41,795 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:41,795 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change null -> WORKER_STARTED
2022-06-22T11:44:41,795 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change null -> WORKER_STARTED
2022-06-22T11:44:41,797 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:41,797 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:41,801 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:41,802 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873081802
2022-06-22T11:44:41,802 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873081802
2022-06-22T11:44:41,818 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:41,878 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:41,878 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:41,879 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:41,879 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:44:41,884 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:41,889 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:41,889 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:41,889 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.image_handler'
2022-06-22T11:44:41,879 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:41,879 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:41,892 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:41,892 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:41,892 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:41,892 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:41,892 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:41,892 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:41,893 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:41,893 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:41,893 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:44:41,893 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:44:41,898 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:41,898 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:41,898 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:41,898 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:42,895 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:42,895 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27217
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:43,198 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:43,198 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:43,199 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873083199
2022-06-22T11:44:43,199 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873083199
2022-06-22T11:44:43,199 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:43,208 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:43,266 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:43,266 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:43,266 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:43,266 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:43,266 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:43,266 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:43,267 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:43,267 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:43,272 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:43,272 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:44,269 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:44,269 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27222
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:44,592 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:44,592 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:44,593 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873084593
2022-06-22T11:44:44,593 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873084593
2022-06-22T11:44:44,593 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:44,602 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:44,663 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:44,663 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:44,663 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:44,663 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:44,663 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:44,663 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:44,669 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:44,669 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:46,665 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:46,665 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:46,974 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27268
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:46,975 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:46,975 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:46,976 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:46,976 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873086976
2022-06-22T11:44:46,976 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873086976
2022-06-22T11:44:46,984 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:47,042 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:47,042 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:47,042 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:47,042 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:47,042 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:47,042 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:47,043 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:47,043 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:47,048 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:47,048 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:50,044 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:50,044 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:50,351 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:50,351 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27286
2022-06-22T11:44:50,351 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:50,352 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:50,352 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873090352
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873090352
2022-06-22T11:44:50,361 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:50,422 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:50,422 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:50,422 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:50,422 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:50,422 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:50,422 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:44:50,423 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:50,423 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:50,428 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:50,428 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:52,746 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /127.0.0.1:59540 "POST /predictions/mnist-net HTTP/1.1" 404 4
2022-06-22T11:44:52,746 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873092
2022-06-22T11:44:55,424 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:55,424 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27307
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:55,740 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:55,740 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:55,741 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:55,742 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873095742
2022-06-22T11:44:55,742 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873095742
2022-06-22T11:44:55,750 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:55,810 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:55,810 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:55,810 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:55,810 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:55,810 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:55,810 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:44:55,810 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:55,810 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:44:55,811 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:55,811 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:55,811 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:55,811 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:55,811 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:55,811 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:55,816 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:55,816 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:56,760 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:59542 "POST /predictions/mnistnet HTTP/1.1" 404 0
2022-06-22T11:44:56,761 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873092
2022-06-22T11:45:03,812 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:03,812 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:04,119 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:45:04,119 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27321
2022-06-22T11:45:04,120 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:45:04,120 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:45:04,120 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:04,120 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:04,120 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:04,120 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:04,121 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873104121
2022-06-22T11:45:04,121 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873104121
2022-06-22T11:45:04,121 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:45:04,134 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:45:04,195 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:45:04,195 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:45:04,196 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:45:04,196 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:45:04,202 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:04,202 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:17,198 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:17,198 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27376
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:45:17,515 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:17,515 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:17,516 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873117516
2022-06-22T11:45:17,516 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:45:17,516 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873117516
2022-06-22T11:45:17,526 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:45:17,587 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:45:17,587 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:17,587 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:45:17,587 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:45:17,588 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:17,588 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:45:17,588 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:17,588 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:45:17,593 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:17,593 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:23,627 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /127.0.0.1:59544 "OPTIONS / HTTP/1.1" 200 8
2022-06-22T11:45:23,627 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873092
2022-06-22T11:45:38,589 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:38,589 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27430
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:45:38,895 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:38,895 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:38,896 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:45:38,896 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873138896
2022-06-22T11:45:38,896 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873138896
2022-06-22T11:45:38,907 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:45:38,967 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:45:38,967 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:45:38,967 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:38,967 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:45:38,967 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:45:38,967 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:45:38,968 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:38,968 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:38,973 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:38,973 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03687286376953|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.64027404785156|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.0580912863070537|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:248|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10572.66796875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,785 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4801.36328125|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,785 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:33.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:46:12,969 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:46:12,969 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:46:13,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:46:13,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27455
2022-06-22T11:46:13,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:46:13,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:46:13,267 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:46:13,267 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:46:13,267 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:46:13,267 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:46:13,268 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873173268
2022-06-22T11:46:13,268 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:46:13,268 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873173268
2022-06-22T11:46:13,277 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:46:13,337 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:46:13,337 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:46:13,343 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:46:13,343 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:47:08,339 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:47:08,339 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:47:08,639 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27488
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:47:08,640 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:47:08,640 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:47:08,641 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:47:08,641 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873228641
2022-06-22T11:47:08,641 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873228641
2022-06-22T11:47:08,652 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:47:08,713 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:47:08,713 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-06-22T11:47:08,719 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:47:08,719 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:48:37,716 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:48:37,716 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27582
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:48:38,027 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:48:38,027 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:48:38,028 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:48:38,028 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873318028
2022-06-22T11:48:38,028 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873318028
2022-06-22T11:48:38,036 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:48:38,097 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:48:38,097 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:48:38,097 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:48:38,097 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:48:38,097 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:48:38,097 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:48:38,098 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:48:38,098 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2022-06-22T11:48:38,103 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:48:38,103 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:49:28,759 [INFO ] W-9000-net_1.0 ACCESS_LOG - /127.0.0.1:59550 "GET /ping HTTP/1.1" 200 3
2022-06-22T11:49:28,760 [INFO ] W-9000-net_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873092
2022-06-22T11:49:59,769 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:49:59,769 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:49:59,851 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet=mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:49:59,851 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet=mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:49:59,862 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:49:59,862 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:49:59,873 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:49:59,873 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T11:49:59,885 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T11:49:59,885 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T11:49:59,890 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:49:59,890 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:49:59,890 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:49:59,890 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:49:59,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:49:59,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:49:59,924 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:49:59,924 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:50:00,010 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:50:00,010 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:50:00,210 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03976440429688|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63738250732422|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:1.9087136929460582|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:230|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:8|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,212 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:11657.88671875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,212 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3757.97265625|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,212 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:26.4|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,233 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:00,234 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27887
2022-06-22T11:50:00,234 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:00,234 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:00,234 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T11:50:00,234 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T11:50:00,236 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:00,236 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:00,241 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:00,242 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873400242
2022-06-22T11:50:00,242 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873400242
2022-06-22T11:50:00,258 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:00,316 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:00,316 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:00,316 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:00,316 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:00,317 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:00,317 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:00,317 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:00,317 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.image_handler'
2022-06-22T11:50:00,317 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:00,317 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:00,331 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:00,331 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:00,331 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:50:00,331 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:50:00,337 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:00,337 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:00,337 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:00,337 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:01,333 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:01,333 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:01,634 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:01,634 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27907
2022-06-22T11:50:01,634 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:01,634 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:01,634 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:01,634 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:01,635 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:01,635 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:01,636 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873401636
2022-06-22T11:50:01,636 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873401636
2022-06-22T11:50:01,636 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:01,645 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:01,703 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:01,703 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:01,703 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:01,703 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:01,704 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:01,704 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:01,704 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:01,704 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:50:01,709 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:01,709 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:02,706 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:02,706 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27911
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:03,003 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:03,003 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:03,004 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:03,005 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873403005
2022-06-22T11:50:03,005 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873403005
2022-06-22T11:50:03,013 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:03,070 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:03,070 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:03,070 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:03,070 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:03,071 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:03,071 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:03,071 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:03,071 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:03,072 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:03,072 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:03,077 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:03,077 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:05,073 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:05,073 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:05,386 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27915
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:05,387 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:05,387 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:05,388 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873405388
2022-06-22T11:50:05,388 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873405388
2022-06-22T11:50:05,388 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:05,397 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:05,455 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:05,455 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:05,455 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:05,455 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:05,455 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:05,455 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:05,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:05,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:50:05,461 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:05,461 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:08,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:08,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:08,793 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:08,793 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27995
2022-06-22T11:50:08,793 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:08,794 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:08,794 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:08,794 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:08,794 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:08,794 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:08,795 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873408795
2022-06-22T11:50:08,795 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873408795
2022-06-22T11:50:08,795 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:08,805 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:08,877 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:08,877 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:50:08,883 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:08,883 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:10,293 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59552 "GET /ping HTTP/1.1" 200 4
2022-06-22T11:50:10,294 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873410
2022-06-22T11:50:13,879 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:13,879 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:14,185 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28025
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:14,191 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:14,191 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:14,193 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:14,193 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873414193
2022-06-22T11:50:14,193 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873414193
2022-06-22T11:50:14,202 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:14,260 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:14,261 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:14,261 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:14,261 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:14,261 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:14,261 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:14,261 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:14,262 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:14,262 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:14,267 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:14,267 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:22,263 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:22,263 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28036
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:22,562 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:22,562 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:22,563 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:22,563 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873422563
2022-06-22T11:50:22,563 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873422563
2022-06-22T11:50:22,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:22,637 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:22,637 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:22,637 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:22,637 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:22,638 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:22,638 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:50:22,643 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:22,643 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:35,640 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:35,640 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28048
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:35,939 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:35,939 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:35,941 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:35,941 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873435941
2022-06-22T11:50:35,941 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873435941
2022-06-22T11:50:35,951 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:36,010 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:36,011 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:36,011 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:36,012 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:36,012 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:50:36,012 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:36,012 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:50:36,017 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:36,017 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:57,013 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:57,013 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28057
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:57,323 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:57,323 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:57,324 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873457324
2022-06-22T11:50:57,324 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873457324
2022-06-22T11:50:57,324 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:57,334 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:57,394 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:57,394 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:57,394 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:57,394 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:50:57,394 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:57,394 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:50:57,395 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:57,395 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:50:57,399 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:57,399 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03933334350586|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63781356811523|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:1.4771784232365146|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:178|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:11387.92578125|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4025.8046875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:28.1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:31,396 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:51:31,396 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28076
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:51:31,725 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:51:31,725 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:51:31,726 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:51:31,726 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873491726
2022-06-22T11:51:31,726 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873491726
2022-06-22T11:51:31,737 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:51:31,798 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:51:31,799 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:51:31,799 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:51:31,804 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:51:31,804 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:55,486 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:54:55,486 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:54:55,553 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet,=,mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:54:55,553 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet,=,mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:54:55,556 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:54:55,556 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:54:55,568 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet
2022-06-22T11:54:55,568 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet
2022-06-22T11:54:55,569 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnistnet
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnistnet
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T11:54:55,569 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnistnet
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnistnet
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T11:54:55,574 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:54:55,574 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T11:54:55,586 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T11:54:55,586 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T11:54:55,590 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:55,590 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:55,591 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:54:55,591 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:54:55,622 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:54:55,622 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:54:55,622 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:54:55,622 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:54:55,709 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:54:55,709 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:54:55,939 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03671646118164|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.64043045043945|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.107883817427386|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:254|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:5|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,941 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10955.57421875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,941 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4439.671875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,941 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:30.9|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,948 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:54:55,948 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29143
2022-06-22T11:54:55,948 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:54:55,949 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:54:55,949 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T11:54:55,949 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T11:54:55,951 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:55,951 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:55,955 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:54:55,956 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873695956
2022-06-22T11:54:55,956 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873695956
2022-06-22T11:54:55,972 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:54:56,034 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:54:56,035 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:54:56,035 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:54:56,036 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:56,036 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:56,036 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:54:56,036 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:54:56,036 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:54:56,041 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:54:56,041 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:56,041 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:54:56,041 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:54:56,042 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:56,042 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:54:56,042 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:56,042 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:56,042 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:54:56,042 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:54:56,048 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:56,048 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:57,047 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:57,047 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:57,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:54:57,354 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29163
2022-06-22T11:54:57,354 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:54:57,354 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:54:57,354 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:54:57,354 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873697355
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873697355
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:54:57,364 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:54:57,423 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:54:57,423 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:54:57,423 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:54:57,423 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:57,423 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:54:57,423 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:57,429 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:57,429 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:58,426 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:58,426 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:58,740 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:54:58,740 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29167
2022-06-22T11:54:58,741 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:54:58,741 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:54:58,741 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:54:58,741 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:54:58,741 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:58,741 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:58,742 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873698742
2022-06-22T11:54:58,742 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873698742
2022-06-22T11:54:58,742 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:54:58,750 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:54:58,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:54:58,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:54:58,810 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:54:58,810 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:54:58,812 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:58,812 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:58,816 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:58,816 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:00,813 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:00,813 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:01,127 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29175
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:01,128 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:01,128 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:01,129 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873701129
2022-06-22T11:55:01,129 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873701129
2022-06-22T11:55:01,129 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:01,137 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:01,197 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:01,197 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:01,197 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:01,197 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:01,197 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:01,197 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:01,197 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:01,197 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:01,198 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:01,198 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:01,198 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:01,198 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:01,198 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:01,198 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:55:01,203 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:01,203 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:04,199 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:04,199 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:04,506 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29179
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:04,507 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:04,507 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:04,508 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:04,508 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873704508
2022-06-22T11:55:04,508 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873704508
2022-06-22T11:55:04,516 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:04,577 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:04,577 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:04,577 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:04,577 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:04,577 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:04,577 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:04,578 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:04,578 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:55:04,583 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:04,583 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:07,263 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59554 "GET /ping HTTP/1.1" 200 4
2022-06-22T11:55:07,263 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873707
2022-06-22T11:55:09,579 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:09,579 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29192
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:09,899 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:09,899 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:09,900 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:09,900 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873709900
2022-06-22T11:55:09,900 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873709900
2022-06-22T11:55:09,909 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:09,969 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:09,969 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:09,969 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:09,969 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:09,969 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:09,969 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:09,970 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:09,970 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:55:09,976 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:09,976 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:17,390 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59556 "GET /ping HTTP/1.1" 200 1
2022-06-22T11:55:17,390 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873707
2022-06-22T11:55:17,971 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:17,971 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29222
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:18,271 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:18,271 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:18,272 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873718272
2022-06-22T11:55:18,272 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873718272
2022-06-22T11:55:18,272 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:18,286 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:18,347 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:18,347 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:55:18,354 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:18,354 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:31,349 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:31,349 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29257
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:31,687 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:31,687 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:31,688 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873731688
2022-06-22T11:55:31,688 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:31,688 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873731688
2022-06-22T11:55:31,698 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:31,762 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:31,762 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:31,762 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:31,762 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:55:31,763 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:55:31,763 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:55:31,763 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:31,763 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:31,768 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:31,768 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:52,764 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:52,764 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29324
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:53,114 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:53,114 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:53,115 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:53,115 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873753115
2022-06-22T11:55:53,115 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873753115
2022-06-22T11:55:53,127 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:53,189 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:53,189 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:53,194 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:53,194 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:198.0358543395996|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:235.64129257202148|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:2.8215767634854774|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:340|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:33|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,926 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:10472.6328125|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,926 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:4830.06640625|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,926 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:33.9|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:56:27,191 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:56:27,191 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29447
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:56:27,550 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:56:27,550 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:56:27,551 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:56:27,551 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873787551
2022-06-22T11:56:27,551 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873787551
2022-06-22T11:56:27,560 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:56:27,623 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:56:27,623 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:56:27,623 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:56:27,624 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:56:27,624 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:56:27,629 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:56:27,629 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:56:40,030 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59558 "GET /ping HTTP/1.1" 200 0
2022-06-22T11:56:40,030 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873707
