2022-06-22T11:36:24,986 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:36:24,986 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:36:25,059 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: net=mnist_net.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:36:25,059 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: net=mnist_net.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:36:25,066 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:36:25,066 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:36:25,076 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist_net.mar
2022-06-22T11:36:25,076 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist_net.mar
2022-06-22T11:36:25,077 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnist_net.mar
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnist_net.mar
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T11:36:25,077 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnist_net.mar
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnist_net.mar
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T11:36:25,083 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:36:25,083 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:36:25,112 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:36:25,112 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:36:25,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:36:25,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:36:25,113 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:36:25,232 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-22T11:36:25,232 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-22T11:37:25,461 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,461 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03841018676758|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,461 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63873672485352|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,462 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,462 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.2987551867219915|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,462 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:277|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,462 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:4|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,463 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10506.5546875|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,463 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4840.34375|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:25,463 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:33.7|#Level:Host|#hostname:hello-world-pc,timestamp:1655872645
2022-06-22T11:37:39,472 [INFO ] pool-2-thread-1 ACCESS_LOG - /127.0.0.1:59516 "GET /ping HTTP/1.1" 200 5
2022-06-22T11:37:39,473 [INFO ] pool-2-thread-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03882598876953|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63832092285156|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.680497925311203|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,423 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:323|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,424 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:13|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,424 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10464.32421875|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,424 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4857.859375|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:38:25,424 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:34.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872705
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.02838516235352|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.64876174926758|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.672199170124481|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:322|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:17|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,426 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10421.7109375|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,427 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4916.0859375|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:39:25,427 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:34.2|#Level:Host|#hostname:hello-world-pc,timestamp:1655872765
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.0410499572754|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.6360969543457|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.9377593360995853|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,436 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:354|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,437 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:41|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,437 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10652.80078125|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,437 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4685.05078125|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:25,437 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:32.8|#Level:Host|#hostname:hello-world-pc,timestamp:1655872825
2022-06-22T11:40:59,570 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:59518 "POST /predictions/net HTTP/1.1" 404 3
2022-06-22T11:40:59,571 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:05,702 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /127.0.0.1:59520 "GET /predictions/net HTTP/1.1" 404 1
2022-06-22T11:41:05,703 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:05,774 [INFO ] epollEventLoopGroup-3-4 ACCESS_LOG - /127.0.0.1:59522 "GET /favicon.ico HTTP/1.1" 404 0
2022-06-22T11:41:05,775 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:08,282 [INFO ] epollEventLoopGroup-3-5 ACCESS_LOG - /127.0.0.1:59524 "GET /predictions/ HTTP/1.1" 404 0
2022-06-22T11:41:08,282 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:15,491 [INFO ] epollEventLoopGroup-3-6 ACCESS_LOG - /127.0.0.1:59526 "GET / HTTP/1.1" 405 0
2022-06-22T11:41:15,491 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:19,223 [INFO ] epollEventLoopGroup-3-7 ACCESS_LOG - /127.0.0.1:59528 "GET / HTTP/1.1" 405 0
2022-06-22T11:41:19,223 [INFO ] epollEventLoopGroup-3-7 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03789520263672|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63925170898438|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,420 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.854771784232365|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:344|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:13|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10694.8515625|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4646.97265625|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:41:25,421 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:32.5|#Level:Host|#hostname:hello-world-pc,timestamp:1655872885
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03767013549805|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63947677612305|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.20746887966805|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:266|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,400 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10756.32421875|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,401 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4602.94140625|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:25,401 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:32.1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872945
2022-06-22T11:42:26,370 [INFO ] epollEventLoopGroup-3-9 ACCESS_LOG - /127.0.0.1:59532 "POST /predictions/mnist-net HTTP/1.1" 404 1
2022-06-22T11:42:26,372 [INFO ] epollEventLoopGroup-3-9 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:43:25,396 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,396 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03764724731445|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63949966430664|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.199170124481328|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:265|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:2|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10759.18359375|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4606.359375|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:25,397 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:32.1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873005
2022-06-22T11:43:30,127 [INFO ] epollEventLoopGroup-3-10 ACCESS_LOG - /127.0.0.1:59534 "POST /predictions/net HTTP/1.1" 404 1
2022-06-22T11:43:30,127 [INFO ] epollEventLoopGroup-3-10 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:43:40,427 [INFO ] epollEventLoopGroup-3-11 ACCESS_LOG - /127.0.0.1:59536 "POST /predictions/mnist_net HTTP/1.1" 404 0
2022-06-22T11:43:40,428 [INFO ] epollEventLoopGroup-3-11 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:43:45,182 [INFO ] epollEventLoopGroup-3-12 ACCESS_LOG - /127.0.0.1:59538 "POST /predictions/mnist-net HTTP/1.1" 404 1
2022-06-22T11:43:45,182 [INFO ] epollEventLoopGroup-3-12 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655872659
2022-06-22T11:44:41,353 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:44:41,353 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:44:41,431 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: net=mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:44:41,431 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: net=mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:44:41,440 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:44:41,440 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:44:41,452 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:44:41,452 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model net
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model net
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model net
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model net
2022-06-22T11:44:41,464 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model net loaded.
2022-06-22T11:44:41,464 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model net loaded.
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: net, count: 1
2022-06-22T11:44:41,464 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: net, count: 1
2022-06-22T11:44:41,468 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:41,468 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:41,469 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:44:41,469 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:44:41,498 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:44:41,498 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:44:41,499 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:44:41,585 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:44:41,585 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:44:41,790 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,790 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03741836547852|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,790 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63972854614258|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.12448132780083|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:256|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:16|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10802.578125|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,791 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4562.96484375|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,792 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:31.8|#Level:Host|#hostname:hello-world-pc,timestamp:1655873081
2022-06-22T11:44:41,794 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:41,794 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27197
2022-06-22T11:44:41,795 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:41,795 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:41,795 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change null -> WORKER_STARTED
2022-06-22T11:44:41,795 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change null -> WORKER_STARTED
2022-06-22T11:44:41,797 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:41,797 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:41,801 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:41,802 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873081802
2022-06-22T11:44:41,802 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873081802
2022-06-22T11:44:41,818 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:41,878 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:41,878 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:41,878 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:41,879 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:41,879 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:41,879 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:44:41,880 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:44:41,884 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:44:41,885 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:41,886 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:41,889 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:41,889 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:41,889 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T11:44:41,890 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.image_handler'
2022-06-22T11:44:41,879 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:41,879 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:41,892 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:41,892 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:41,892 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:41,892 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:41,892 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:41,892 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:41,893 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:41,893 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:41,893 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:44:41,893 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:44:41,898 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:41,898 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:41,898 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:41,898 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:42,895 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:42,895 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27217
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:43,198 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:43,198 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:43,198 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:43,199 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873083199
2022-06-22T11:44:43,199 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873083199
2022-06-22T11:44:43,199 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:43,208 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:43,266 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:43,266 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:43,266 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:43,266 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:43,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:43,266 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:43,266 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:43,267 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:43,267 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:43,267 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:44:43,267 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:43,272 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:43,272 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:44,269 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:44,269 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27222
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:44,592 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:44,592 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:44,592 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:44,593 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873084593
2022-06-22T11:44:44,593 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873084593
2022-06-22T11:44:44,593 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:44,602 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:44,663 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:44,663 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:44,663 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:44,663 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:44,663 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:44,663 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:44,664 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:44,664 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:44,664 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:44,669 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:44,669 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:46,665 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:46,665 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:46,974 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27268
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:46,975 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:46,975 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:46,975 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:46,976 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:46,976 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873086976
2022-06-22T11:44:46,976 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873086976
2022-06-22T11:44:46,984 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:47,042 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:47,042 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:47,042 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:47,042 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:47,042 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:47,042 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:47,042 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:47,043 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:47,043 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:47,043 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:47,043 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:47,048 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:47,048 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:50,044 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:50,044 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:50,351 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:50,351 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27286
2022-06-22T11:44:50,351 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:50,352 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:50,352 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873090352
2022-06-22T11:44:50,352 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873090352
2022-06-22T11:44:50,361 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:50,422 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:50,422 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:50,422 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:50,422 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:50,422 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:50,422 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:50,422 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:44:50,423 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:50,423 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:50,423 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:44:50,423 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:50,428 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:50,428 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:52,746 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /127.0.0.1:59540 "POST /predictions/mnist-net HTTP/1.1" 404 4
2022-06-22T11:44:52,746 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873092
2022-06-22T11:44:55,424 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:55,424 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27307
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:44:55,740 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:55,740 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:55,740 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:44:55,741 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:44:55,742 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873095742
2022-06-22T11:44:55,742 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873095742
2022-06-22T11:44:55,750 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:44:55,810 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:44:55,810 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:44:55,810 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:44:55,810 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:44:55,810 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:44:55,810 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:44:55,810 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:44:55,810 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:55,810 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:44:55,811 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:55,811 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:44:55,811 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:55,811 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:44:55,811 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:55,811 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:44:55,811 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:44:55,816 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:55,816 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:44:56,760 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:59542 "POST /predictions/mnistnet HTTP/1.1" 404 0
2022-06-22T11:44:56,761 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873092
2022-06-22T11:45:03,812 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:03,812 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:04,119 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:45:04,119 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27321
2022-06-22T11:45:04,120 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:45:04,120 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:45:04,120 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:04,120 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:04,120 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:04,120 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:04,121 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873104121
2022-06-22T11:45:04,121 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873104121
2022-06-22T11:45:04,121 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:45:04,134 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:45:04,195 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:45:04,195 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:45:04,196 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:45:04,196 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:04,196 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:04,196 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:04,196 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:45:04,202 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:04,202 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:17,198 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:17,198 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27376
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:45:17,515 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:17,515 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:17,515 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:17,516 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873117516
2022-06-22T11:45:17,516 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:45:17,516 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873117516
2022-06-22T11:45:17,526 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:45:17,587 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:45:17,587 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:17,587 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:45:17,587 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:17,587 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:17,587 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:45:17,588 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:17,588 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:45:17,588 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:17,588 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:45:17,588 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:45:17,593 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:17,593 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:23,627 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /127.0.0.1:59544 "OPTIONS / HTTP/1.1" 200 8
2022-06-22T11:45:23,627 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873092
2022-06-22T11:45:38,589 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:38,589 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27430
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:45:38,895 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:38,895 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:38,895 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:45:38,896 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:45:38,896 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873138896
2022-06-22T11:45:38,896 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873138896
2022-06-22T11:45:38,907 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:45:38,967 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:45:38,967 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:45:38,967 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:45:38,967 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:45:38,967 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:45:38,967 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:45:38,967 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:45:38,968 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:38,968 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:38,968 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:45:38,968 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:45:38,973 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:38,973 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03687286376953|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.64027404785156|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.0580912863070537|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:248|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,784 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10572.66796875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,785 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4801.36328125|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:45:41,785 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:33.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873141
2022-06-22T11:46:12,969 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:46:12,969 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:46:13,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:46:13,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27455
2022-06-22T11:46:13,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:46:13,266 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:46:13,267 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:46:13,267 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:46:13,267 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:46:13,267 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:46:13,268 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873173268
2022-06-22T11:46:13,268 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:46:13,268 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873173268
2022-06-22T11:46:13,277 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:46:13,337 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:46:13,337 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:46:13,337 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:46:13,338 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:46:13,338 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:46:13,338 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:46:13,343 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:46:13,343 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:47:08,339 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:47:08,339 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:47:08,639 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27488
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:47:08,640 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:47:08,640 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:47:08,640 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:47:08,641 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:47:08,641 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873228641
2022-06-22T11:47:08,641 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873228641
2022-06-22T11:47:08,652 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:47:08,713 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:47:08,713 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:47:08,713 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:47:08,714 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:47:08,714 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:47:08,714 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-06-22T11:47:08,719 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:47:08,719 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:48:37,716 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:48:37,716 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - [PID]27582
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:48:38,027 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:48:38,027 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:48:38,027 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:48:38,028 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:48:38,028 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873318028
2022-06-22T11:48:38,028 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873318028
2022-06-22T11:48:38,036 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - model_name: net, batchSize: 1
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:48:38,097 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:48:38,097 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/tmp/models/fd7d9c8cc16348d6b5f3ee6c92e3fd72/image_handler.py", line 3, in <module>
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:48:38,097 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:48:38,097 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:48:38,097 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - 
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:48:38,097 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:48:38,097 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: net, error: Worker died.
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:48:38,098 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:48:38,098 [DEBUG] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-net_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stderr
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:48:38,098 [WARN ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-net_1.0-stdout
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stdout
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2022-06-22T11:48:38,098 [INFO ] W-9000-net_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2022-06-22T11:48:38,103 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:48:38,103 [INFO ] W-9000-net_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-net_1.0-stderr
2022-06-22T11:49:28,759 [INFO ] W-9000-net_1.0 ACCESS_LOG - /127.0.0.1:59550 "GET /ping HTTP/1.1" 200 3
2022-06-22T11:49:28,760 [INFO ] W-9000-net_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873092
2022-06-22T11:49:59,769 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:49:59,769 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:49:59,851 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet=mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:49:59,851 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet=mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:49:59,862 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:49:59,862 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:49:59,873 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:49:59,873 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T11:49:59,885 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T11:49:59,885 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T11:49:59,885 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T11:49:59,890 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:49:59,890 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:49:59,890 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:49:59,890 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:49:59,923 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:49:59,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:49:59,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:49:59,924 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:49:59,924 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:50:00,010 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:50:00,010 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:50:00,210 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03976440429688|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63738250732422|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:1.9087136929460582|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:230|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:8|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,212 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:11657.88671875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,212 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3757.97265625|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,212 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:26.4|#Level:Host|#hostname:hello-world-pc,timestamp:1655873400
2022-06-22T11:50:00,233 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:00,234 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27887
2022-06-22T11:50:00,234 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:00,234 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:00,234 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T11:50:00,234 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T11:50:00,236 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:00,236 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:00,241 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:00,242 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873400242
2022-06-22T11:50:00,242 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873400242
2022-06-22T11:50:00,258 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:00,316 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:00,316 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:00,316 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:00,316 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:00,317 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:00,317 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:00,317 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:00,317 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:00,317 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:00,318 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:50:00,319 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T11:50:00,320 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.image_handler'
2022-06-22T11:50:00,317 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:00,317 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:00,331 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:00,331 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:00,331 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:00,331 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:50:00,331 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:50:00,337 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:00,337 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:00,337 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:00,337 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:01,333 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:01,333 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:01,634 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:01,634 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27907
2022-06-22T11:50:01,634 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:01,634 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:01,634 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:01,634 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:01,635 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:01,635 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:01,636 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873401636
2022-06-22T11:50:01,636 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873401636
2022-06-22T11:50:01,636 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:01,645 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:01,703 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:01,703 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:01,703 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:01,703 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:01,703 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:01,704 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:01,704 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:01,704 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:01,704 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:01,704 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:01,704 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:50:01,709 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:01,709 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:02,706 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:02,706 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27911
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:03,003 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:03,003 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:03,003 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:03,004 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:03,005 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873403005
2022-06-22T11:50:03,005 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873403005
2022-06-22T11:50:03,013 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:03,070 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:03,070 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:03,070 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:03,070 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:03,071 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:03,071 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:03,071 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:03,071 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:03,071 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:03,071 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:03,072 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:03,072 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:03,072 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:03,077 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:03,077 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:05,073 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:05,073 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:05,386 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27915
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:05,387 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:05,387 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:05,387 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:05,388 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873405388
2022-06-22T11:50:05,388 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873405388
2022-06-22T11:50:05,388 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:05,397 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:05,455 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:05,455 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:05,455 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:05,455 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:05,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:05,455 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:05,455 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:05,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:05,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:05,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:50:05,456 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:50:05,461 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:05,461 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:08,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:08,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:08,793 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:08,793 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]27995
2022-06-22T11:50:08,793 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:08,794 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:08,794 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:08,794 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:08,794 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:08,794 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:08,795 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873408795
2022-06-22T11:50:08,795 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873408795
2022-06-22T11:50:08,795 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:08,805 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:08,877 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:08,877 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:08,877 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:08,878 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:08,878 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:50:08,878 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:50:08,883 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:08,883 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:10,293 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59552 "GET /ping HTTP/1.1" 200 4
2022-06-22T11:50:10,294 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873410
2022-06-22T11:50:13,879 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:13,879 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:14,185 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28025
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:14,191 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:14,191 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:14,191 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:14,193 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:14,193 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873414193
2022-06-22T11:50:14,193 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873414193
2022-06-22T11:50:14,202 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:14,260 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:14,261 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:14,261 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:14,261 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:14,261 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:14,261 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:14,261 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:14,261 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:14,262 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:14,262 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:14,262 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:14,262 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:14,267 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:14,267 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:22,263 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:22,263 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28036
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:22,562 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:22,562 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:22,562 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:22,563 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:22,563 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873422563
2022-06-22T11:50:22,563 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873422563
2022-06-22T11:50:22,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:22,637 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:22,637 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:22,637 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:22,637 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:22,638 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:22,638 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:22,638 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:22,638 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:22,638 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:50:22,643 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:22,643 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:35,640 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:35,640 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28048
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:35,939 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:35,939 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:35,939 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:35,941 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:35,941 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873435941
2022-06-22T11:50:35,941 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873435941
2022-06-22T11:50:35,951 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:36,010 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:36,011 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:36,011 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:36,011 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:36,011 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:36,011 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:36,012 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:36,012 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:50:36,012 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:36,012 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:50:36,017 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:36,017 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:57,013 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:57,013 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28057
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:50:57,323 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:57,323 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:57,323 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:50:57,324 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873457324
2022-06-22T11:50:57,324 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873457324
2022-06-22T11:50:57,324 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:50:57,334 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:50:57,394 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:50:57,394 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:50:57,394 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:57,394 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:57,394 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:50:57,394 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:57,394 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:57,394 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:50:57,395 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:57,395 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:50:57,395 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:50:57,399 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:50:57,399 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03933334350586|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.63781356811523|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,210 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:1.4771784232365146|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:178|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:11387.92578125|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4025.8046875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:00,211 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:28.1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873460
2022-06-22T11:51:31,396 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:51:31,396 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]28076
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:51:31,725 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:51:31,725 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:51:31,725 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:51:31,726 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:51:31,726 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873491726
2022-06-22T11:51:31,726 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873491726
2022-06-22T11:51:31,737 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:51:31,798 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:51:31,799 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:51:31,799 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/7032e0bf84dd475cbf107b6fd338cee6/image_handler.py", line 3, in <module>
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:51:31,799 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:51:31,799 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:51:31,799 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:51:31,804 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:51:31,804 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:55,486 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:54:55,486 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T11:54:55,553 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet,=,mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:54:55,553 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet,=,mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T11:54:55,556 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:54:55,556 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T11:54:55,568 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet
2022-06-22T11:54:55,568 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet
2022-06-22T11:54:55,569 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnistnet
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnistnet
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T11:54:55,569 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnistnet
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnistnet
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T11:54:55,574 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:54:55,574 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T11:54:55,586 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T11:54:55,586 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T11:54:55,586 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T11:54:55,590 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:55,590 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:55,591 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:54:55,591 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:54:55,621 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T11:54:55,622 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:54:55,622 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T11:54:55,622 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:54:55,622 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T11:54:55,709 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:54:55,709 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T11:54:55,939 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.03671646118164|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.64043045043945|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.107883817427386|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:254|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,940 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:5|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,941 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10955.57421875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,941 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4439.671875|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,941 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:30.9|#Level:Host|#hostname:hello-world-pc,timestamp:1655873695
2022-06-22T11:54:55,948 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:54:55,948 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29143
2022-06-22T11:54:55,948 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:54:55,949 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:54:55,949 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T11:54:55,949 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T11:54:55,951 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:55,951 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:55,955 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:54:55,956 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873695956
2022-06-22T11:54:55,956 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873695956
2022-06-22T11:54:55,972 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:54:56,034 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:54:56,035 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:54:56,035 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:54:56,035 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:54:56,036 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:56,036 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:56,036 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:54:56,040 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:54:56,036 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:54:56,036 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:54:56,041 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:54:56,041 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:56,041 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:54:56,041 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:54:56,041 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:54:56,042 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:56,042 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:54:56,042 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:56,042 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:56,042 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:54:56,042 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:54:56,045 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:54:56,048 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:56,048 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:57,047 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:57,047 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:57,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:54:57,354 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29163
2022-06-22T11:54:57,354 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:54:57,354 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:54:57,354 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:54:57,354 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873697355
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873697355
2022-06-22T11:54:57,355 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:54:57,364 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:54:57,423 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:54:57,423 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:54:57,423 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:54:57,423 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:57,423 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:54:57,423 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:57,424 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:57,424 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:57,424 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:57,429 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:57,429 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:58,426 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:58,426 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:54:58,740 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:54:58,740 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29167
2022-06-22T11:54:58,741 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:54:58,741 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:54:58,741 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:54:58,741 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:54:58,741 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:58,741 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:54:58,742 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873698742
2022-06-22T11:54:58,742 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873698742
2022-06-22T11:54:58,742 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:54:58,750 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:54:58,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:54:58,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:54:58,810 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:54:58,810 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:54:58,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:58,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:54:58,811 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-22T11:54:58,812 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:58,812 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:54:58,816 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:54:58,816 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:00,813 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:00,813 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:01,127 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29175
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:01,128 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:01,128 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:01,128 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:01,129 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873701129
2022-06-22T11:55:01,129 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873701129
2022-06-22T11:55:01,129 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:01,137 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:01,197 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:01,197 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:01,197 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:01,197 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:01,197 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:01,197 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:01,197 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:01,197 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:01,197 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:01,198 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:01,198 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:01,198 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:01,198 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:01,198 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:01,198 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:01,198 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-22T11:55:01,203 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:01,203 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:04,199 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:04,199 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:04,506 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29179
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:04,507 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:04,507 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:04,507 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:04,508 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:04,508 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873704508
2022-06-22T11:55:04,508 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873704508
2022-06-22T11:55:04,516 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:04,577 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:04,577 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:04,577 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:04,577 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:04,577 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:04,577 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:04,577 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:04,578 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:04,578 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:04,578 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:04,578 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-22T11:55:04,583 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:04,583 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:07,263 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59554 "GET /ping HTTP/1.1" 200 4
2022-06-22T11:55:07,263 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873707
2022-06-22T11:55:09,579 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:09,579 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29192
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:09,899 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:09,899 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:09,899 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:09,900 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:09,900 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873709900
2022-06-22T11:55:09,900 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873709900
2022-06-22T11:55:09,909 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:09,969 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:09,969 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:09,969 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:09,969 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:09,969 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:09,969 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:09,969 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:09,970 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:09,970 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:09,970 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:55:09,970 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-22T11:55:09,976 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:09,976 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:17,390 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59556 "GET /ping HTTP/1.1" 200 1
2022-06-22T11:55:17,390 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873707
2022-06-22T11:55:17,971 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:17,971 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29222
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:18,271 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:18,271 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:18,271 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:18,272 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873718272
2022-06-22T11:55:18,272 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873718272
2022-06-22T11:55:18,272 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:18,286 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:18,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:18,347 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:18,347 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:18,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:18,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:18,348 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-22T11:55:18,354 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:18,354 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:31,349 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:31,349 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29257
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:31,687 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:31,687 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:31,687 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:31,688 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873731688
2022-06-22T11:55:31,688 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:31,688 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873731688
2022-06-22T11:55:31,698 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:31,762 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:31,762 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:31,762 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:31,762 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:31,762 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:55:31,763 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:55:31,763 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:55:31,763 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:31,763 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:31,763 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:31,763 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:31,768 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:31,768 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:52,764 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:52,764 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29324
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:55:53,114 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:55:53,114 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:53,114 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:55:53,115 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:55:53,115 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873753115
2022-06-22T11:55:53,115 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873753115
2022-06-22T11:55:53,127 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:55:53,188 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:55:53,189 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:55:53,189 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:55:53,189 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:53,189 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:53,189 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:55:53,194 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:53,194 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:198.0358543395996|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:235.64129257202148|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:2.8215767634854774|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:340|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,925 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:33|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,926 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:10472.6328125|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,926 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:4830.06640625|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:55:55,926 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:33.9|#Level:Host|#hostname:hello-world-pc,timestamp:1655873755
2022-06-22T11:56:27,191 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:56:27,191 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29447
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:56:27,550 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:56:27,550 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:56:27,550 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:56:27,551 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:56:27,551 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873787551
2022-06-22T11:56:27,551 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873787551
2022-06-22T11:56:27,560 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:56:27,623 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:56:27,623 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:56:27,623 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:56:27,624 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:56:27,624 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:56:27,624 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:56:27,624 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:56:27,624 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-22T11:56:27,629 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:56:27,629 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:56:40,030 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59558 "GET /ping HTTP/1.1" 200 0
2022-06-22T11:56:40,030 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873707
2022-06-22T11:57:22,625 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:57:22,625 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:57:22,942 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:57:22,942 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29665
2022-06-22T11:57:22,942 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:57:22,942 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:57:22,942 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:57:22,942 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:57:22,942 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:57:22,942 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:57:22,943 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873842943
2022-06-22T11:57:22,943 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:57:22,943 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873842943
2022-06-22T11:57:22,954 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:57:23,017 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:57:23,017 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:57:23,017 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:57:23,017 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:57:23,017 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:57:23,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:57:23,017 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:57:23,018 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:57:23,018 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:57:23,018 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:57:23,018 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:57:23,018 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:57:23,018 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:57:23,018 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:57:23,018 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-06-22T11:57:23,018 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-06-22T11:57:23,023 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:57:23,023 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:58:52,020 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:58:52,020 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T11:58:52,353 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T11:58:52,353 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]29898
2022-06-22T11:58:52,353 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T11:58:52,353 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T11:58:52,353 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:58:52,353 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T11:58:52,354 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:58:52,354 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T11:58:52,355 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T11:58:52,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873932355
2022-06-22T11:58:52,355 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655873932355
2022-06-22T11:58:52,366 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T11:58:52,431 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T11:58:52,431 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:58:52,431 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T11:58:52,431 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T11:58:52,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T11:58:52,431 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T11:58:52,431 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T11:58:52,432 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T11:58:52,432 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T11:58:52,432 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:58:52,432 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T11:58:52,432 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T11:58:52,432 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T11:58:52,432 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:58:52,432 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2022-06-22T11:58:52,432 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2022-06-22T11:58:52,437 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T11:58:52,437 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:00:59,201 [INFO ] pool-2-thread-3 ACCESS_LOG - /127.0.0.1:59560 "GET /ping HTTP/1.1" 200 1
2022-06-22T12:00:59,202 [INFO ] pool-2-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873707
2022-06-22T12:01:16,434 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:01:16,434 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:01:16,770 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T12:01:16,771 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]30169
2022-06-22T12:01:16,771 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T12:01:16,771 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T12:01:16,771 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:01:16,771 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:01:16,771 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:01:16,771 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:01:16,772 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655874076772
2022-06-22T12:01:16,772 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T12:01:16,772 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655874076772
2022-06-22T12:01:16,779 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T12:01:16,843 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T12:01:16,843 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T12:01:16,843 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:01:16,843 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T12:01:16,843 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T12:01:16,843 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T12:01:16,843 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T12:01:16,843 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T12:01:16,843 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:01:16,843 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:01:16,843 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:01:16,843 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T12:01:16,843 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:01:16,844 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:01:16,844 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:01:16,844 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:01:16,844 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:01:16,844 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:01:16,844 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 233 seconds.
2022-06-22T12:01:16,844 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:01:16,844 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 233 seconds.
2022-06-22T12:01:16,848 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:01:16,848 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:05:09,846 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:05:09,846 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:05:10,151 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T12:05:10,151 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]31082
2022-06-22T12:05:10,151 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T12:05:10,151 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T12:05:10,151 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:05:10,151 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:05:10,151 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:05:10,151 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:05:10,152 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T12:05:10,153 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655874310153
2022-06-22T12:05:10,153 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655874310153
2022-06-22T12:05:10,162 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T12:05:10,224 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T12:05:10,224 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T12:05:10,224 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T12:05:10,224 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:05:10,224 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:05:10,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:05:10,224 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T12:05:10,225 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:05:10,225 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:05:10,225 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T12:05:10,225 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T12:05:10,225 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T12:05:10,225 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:05:10,225 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:05:10,225 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 377 seconds.
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 377 seconds.
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:05:10,225 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:05:10,230 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:05:10,230 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:11:27,227 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:11:27,227 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:11:27,526 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T12:11:27,526 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]31569
2022-06-22T12:11:27,526 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T12:11:27,526 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T12:11:27,526 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:11:27,526 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:11:27,526 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:11:27,526 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:11:27,527 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T12:11:27,527 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655874687527
2022-06-22T12:11:27,527 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655874687527
2022-06-22T12:11:27,537 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T12:11:27,598 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T12:11:27,598 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T12:11:27,598 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T12:11:27,598 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:11:27,598 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T12:11:27,598 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.image_handler'
2022-06-22T12:11:27,598 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:11:27,598 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:11:27,598 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:11:27,598 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:11:27,598 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:11:27,598 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:11:27,598 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:11:27,598 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 610 seconds.
2022-06-22T12:11:27,598 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 610 seconds.
2022-06-22T12:11:27,603 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:11:27,603 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:11:27,603 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:11:27,603 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:21:37,601 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:21:37,601 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:21:37,905 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T12:21:37,905 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]32449
2022-06-22T12:21:37,906 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T12:21:37,906 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T12:21:37,906 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:21:37,906 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:21:37,906 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:21:37,906 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:21:37,907 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T12:21:37,907 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655875297907
2022-06-22T12:21:37,907 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655875297907
2022-06-22T12:21:37,917 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T12:21:37,977 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T12:21:37,977 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T12:21:37,977 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T12:21:37,977 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:21:37,977 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T12:21:37,977 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T12:21:37,977 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:21:37,978 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:21:37,978 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:21:37,978 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:21:37,978 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:21:37,978 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T12:21:37,978 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.image_handler'
2022-06-22T12:21:37,978 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:21:37,978 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 987 seconds.
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:21:37,978 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 987 seconds.
2022-06-22T12:21:37,982 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:21:37,982 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:38:04,980 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:38:04,980 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T12:38:05,277 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T12:38:05,277 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]33653
2022-06-22T12:38:05,277 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T12:38:05,277 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T12:38:05,277 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:38:05,277 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T12:38:05,277 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:38:05,277 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T12:38:05,279 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T12:38:05,279 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655876285279
2022-06-22T12:38:05,279 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655876285279
2022-06-22T12:38:05,288 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T12:38:05,347 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T12:38:05,347 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T12:38:05,347 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:38:05,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:38:05,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.image_handler'
2022-06-22T12:38:05,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:38:05,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T12:38:05,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:38:05,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T12:38:05,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:38:05,348 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T12:38:05,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:38:05,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T12:38:05,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:38:05,348 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T12:38:05,348 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T12:38:05,353 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:38:05,353 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T12:38:05,353 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T12:38:05,353 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T13:04:42,350 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T13:04:42,350 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T13:04:42,644 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T13:04:42,644 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]34932
2022-06-22T13:04:42,644 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T13:04:42,644 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T13:04:42,644 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T13:04:42,644 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T13:04:42,645 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T13:04:42,645 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T13:04:42,645 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T13:04:42,646 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655877882646
2022-06-22T13:04:42,646 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655877882646
2022-06-22T13:04:42,657 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T13:04:42,716 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T13:04:42,717 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T13:04:42,717 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T13:04:42,717 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T13:04:42,717 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T13:04:42,717 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T13:04:42,717 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T13:04:42,717 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T13:04:42,717 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T13:04:42,717 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T13:04:42,717 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T13:04:42,717 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T13:04:42,717 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T13:04:42,717 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T13:04:42,717 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T13:04:42,717 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T13:04:42,722 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T13:04:42,722 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T13:31:19,720 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T13:31:19,720 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T13:31:20,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T13:31:20,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]36021
2022-06-22T13:31:20,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T13:31:20,017 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T13:31:20,017 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T13:31:20,017 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T13:31:20,017 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T13:31:20,017 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T13:31:20,019 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T13:31:20,019 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655879480019
2022-06-22T13:31:20,019 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655879480019
2022-06-22T13:31:20,029 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T13:31:20,086 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T13:31:20,086 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T13:31:20,086 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T13:31:20,086 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T13:31:20,086 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T13:31:20,087 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T13:31:20,087 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T13:31:20,087 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T13:31:20,087 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T13:31:20,087 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T13:31:20,087 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T13:31:20,087 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T13:31:20,087 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T13:31:20,087 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T13:31:20,087 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T13:31:20,087 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T13:31:20,087 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T13:31:20,087 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T13:31:20,087 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T13:31:20,087 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T13:31:20,092 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T13:31:20,092 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T13:57:57,090 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T13:57:57,090 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T13:57:57,385 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T13:57:57,385 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]37191
2022-06-22T13:57:57,385 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T13:57:57,385 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T13:57:57,385 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T13:57:57,385 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T13:57:57,386 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T13:57:57,386 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T13:57:57,387 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655881077387
2022-06-22T13:57:57,387 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655881077387
2022-06-22T13:57:57,387 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T13:57:57,397 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T13:57:57,455 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T13:57:57,455 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T13:57:57,455 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T13:57:57,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T13:57:57,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T13:57:57,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T13:57:57,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T13:57:57,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T13:57:57,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T13:57:57,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T13:57:57,456 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T13:57:57,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T13:57:57,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T13:57:57,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T13:57:57,456 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T13:57:57,456 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T13:57:57,461 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T13:57:57,461 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T14:24:34,459 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T14:24:34,459 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T14:24:34,751 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T14:24:34,751 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]39185
2022-06-22T14:24:34,751 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T14:24:34,751 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T14:24:34,751 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T14:24:34,751 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T14:24:34,751 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T14:24:34,751 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T14:24:34,752 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T14:24:34,752 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655882674752
2022-06-22T14:24:34,752 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655882674752
2022-06-22T14:24:34,752 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T14:24:34,810 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T14:24:34,810 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T14:24:34,810 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T14:24:34,810 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T14:24:34,810 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T14:24:34,810 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T14:24:34,810 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T14:24:34,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T14:24:34,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T14:24:34,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T14:24:34,811 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T14:24:34,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T14:24:34,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T14:24:34,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T14:24:34,811 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T14:24:34,811 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T14:24:34,815 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T14:24:34,815 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T14:51:11,813 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T14:51:11,813 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T14:51:12,107 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T14:51:12,108 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]40145
2022-06-22T14:51:12,108 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T14:51:12,108 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T14:51:12,108 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T14:51:12,108 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-22T14:51:12,108 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T14:51:12,108 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T14:51:12,109 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T14:51:12,109 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655884272109
2022-06-22T14:51:12,109 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655884272109
2022-06-22T14:51:12,109 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T14:51:12,166 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-22T14:51:12,166 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T14:51:12,166 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-06-22T14:51:12,166 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-06-22T14:51:12,166 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-06-22T14:51:12,166 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/tmp/models/fbadfc21f2da40d9bf906117261ec2c2/image_handler.py", line 3, in <module>
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-06-22T14:51:12,167 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-06-22T14:51:12,167 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - 
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-22T14:51:12,167 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-22T14:51:12,167 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-06-22T14:51:12,167 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-06-22T14:51:12,167 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T14:51:12,167 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T14:51:12,167 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnistnet, error: Worker died.
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T14:51:12,167 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-06-22T14:51:12,167 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.image_handler'
2022-06-22T14:51:12,167 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T14:51:12,167 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stderr
2022-06-22T14:51:12,167 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T14:51:12,167 [INFO ] W-9000-mnistnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stdout
2022-06-22T14:51:12,167 [WARN ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnistnet_1.0-stdout
2022-06-22T14:51:12,168 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T14:51:12,168 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1597 seconds.
2022-06-22T14:51:12,172 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T14:51:12,172 [INFO ] W-9000-mnistnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnistnet_1.0-stderr
2022-06-22T15:02:10,432 [INFO ] pool-2-thread-13 ACCESS_LOG - /127.0.0.1:59562 "GET /ping HTTP/1.1" 200 1
2022-06-22T15:02:10,433 [INFO ] pool-2-thread-13 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655873707
2022-06-22T15:03:12,737 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T15:03:12,737 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-22T15:03:12,823 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet,=,mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T15:03:12,823 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages
Current directory: /home/nmd2000/Workspace/torchserve-demo
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3962 M
Python executable: /home/nmd2000/miniconda3/envs/ml/bin/python
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Initial Models: mnistnet,=,mnistnet.mar
Log dir: /home/nmd2000/Workspace/torchserve-demo/logs
Metrics dir: /home/nmd2000/Workspace/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/nmd2000/Workspace/torchserve-demo/deployment/model-store
Model config: N/A
2022-06-22T15:03:12,834 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T15:03:12,834 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-22T15:03:12,849 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet
2022-06-22T15:03:12,849 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet
2022-06-22T15:03:12,850 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnistnet
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnistnet
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T15:03:12,850 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: mnistnet
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: mnistnet
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:87) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:167) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:133) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:242) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:356) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:117) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:98) [model-server.jar:?]
2022-06-22T15:03:12,855 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T15:03:12,855 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnistnet.mar
2022-06-22T15:03:12,866 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T15:03:12,866 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnistnet
2022-06-22T15:03:12,866 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T15:03:12,866 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnistnet
2022-06-22T15:03:12,866 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T15:03:12,866 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnistnet loaded.
2022-06-22T15:03:12,866 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T15:03:12,866 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnistnet, count: 1
2022-06-22T15:03:12,870 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T15:03:12,871 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T15:03:12,871 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-22T15:03:12,870 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/nmd2000/miniconda3/envs/ml/bin/python, /home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-22T15:03:12,903 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T15:03:12,903 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-06-22T15:03:12,903 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T15:03:12,903 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-22T15:03:12,904 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T15:03:12,904 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-06-22T15:03:12,904 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T15:03:12,904 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-22T15:03:12,904 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T15:03:12,904 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-06-22T15:03:13,010 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T15:03:13,010 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-22T15:03:13,208 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,209 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.01199340820312|#Level:Host|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,209 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.66515350341797|#Level:Host|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,209 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,209 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.132780082987552|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,209 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:257|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,209 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:31|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,209 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:10488.8359375|#Level:Host|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,209 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:4909.02734375|#Level:Host|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,210 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:33.8|#Level:Host|#hostname:hello-world-pc,timestamp:1655884993
2022-06-22T15:03:13,217 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-22T15:03:13,218 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - [PID]40988
2022-06-22T15:03:13,218 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-22T15:03:13,218 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.12
2022-06-22T15:03:13,218 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T15:03:13,218 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change null -> WORKER_STARTED
2022-06-22T15:03:13,220 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T15:03:13,220 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-22T15:03:13,224 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-22T15:03:13,225 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655884993225
2022-06-22T15:03:13,225 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655884993225
2022-06-22T15:03:13,242 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - model_name: mnistnet, batchSize: 1
2022-06-22T15:03:15,431 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-06-22T15:03:15,440 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2198
2022-06-22T15:03:15,440 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2198
2022-06-22T15:03:15,440 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-22T15:03:15,440 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnistnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-22T15:03:15,441 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - W-9000-mnistnet_1.0.ms:2572|#Level:Host|#hostname:hello-world-pc,timestamp:1655884995
2022-06-22T15:03:15,442 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - WorkerThreadTime.ms:19|#Level:Host|#hostname:hello-world-pc,timestamp:1655884995
2022-06-22T15:03:25,287 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:59564 "GET /ping HTTP/1.1" 200 4
2022-06-22T15:03:25,287 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885005
2022-06-22T15:03:39,419 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:59566 "POST /predictions/net HTTP/1.1" 404 2
2022-06-22T15:03:39,420 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885005
2022-06-22T15:03:47,809 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885027809
2022-06-22T15:03:47,809 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885027809
2022-06-22T15:03:47,811 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend received inference at: 1655885027
2022-06-22T15:03:48,226 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 416
2022-06-22T15:03:48,226 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:415.12|#ModelName:mnistnet,Level:Model|#hostname:hello-world-pc,requestID:27651461-7c91-4a62-a333-22ddd06164e3,timestamp:1655885028
2022-06-22T15:03:48,226 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 416
2022-06-22T15:03:48,227 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:415.16|#ModelName:mnistnet,Level:Model|#hostname:hello-world-pc,requestID:27651461-7c91-4a62-a333-22ddd06164e3,timestamp:1655885028
2022-06-22T15:03:48,227 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59568 "POST /predictions/mnistnet HTTP/1.1" 200 418
2022-06-22T15:03:48,227 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885005
2022-06-22T15:03:48,227 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 264020, Backend time ns: 417882222
2022-06-22T15:03:48,227 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 264020, Backend time ns: 417882222
2022-06-22T15:03:48,227 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885028
2022-06-22T15:03:48,227 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:hello-world-pc,timestamp:1655885028
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.01194381713867|#Level:Host|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.66520309448242|#Level:Host|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:16.240663900414937|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:1957|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:23|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6855.25|#Level:Host|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8516.015625|#Level:Host|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:13,226 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:56.7|#Level:Host|#hostname:hello-world-pc,timestamp:1655885053
2022-06-22T15:04:37,220 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885077220
2022-06-22T15:04:37,220 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885077220
2022-06-22T15:04:37,221 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend received inference at: 1655885077
2022-06-22T15:04:37,245 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-22T15:04:37,245 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-22T15:04:37,245 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 25
2022-06-22T15:04:37,245 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 25
2022-06-22T15:04:37,245 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/service.py", line 102, in predict
2022-06-22T15:04:37,245 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-22T15:04:37,245 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/base_handler.py", line 229, in handle
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59570 "POST /predictions/mnistnet HTTP/1.1" 503 27
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885005
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/ts/torch_handler/vision_handler.py", line 48, in preprocess
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     image = Image.open(io.BytesIO(image))
2022-06-22T15:04:37,246 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 113602, Inference time ns: 26105820
2022-06-22T15:04:37,246 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 113602, Inference time ns: 26105820
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -   File "/home/nmd2000/miniconda3/envs/ml/lib/python3.9/site-packages/PIL/Image.py", line 3008, in open
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG -     raise UnidentifiedImageError(
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885077
2022-06-22T15:04:37,246 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7ff09c4e1900>
2022-06-22T15:05:13,208 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.01214218139648|#Level:Host|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.6650047302246|#Level:Host|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:16.647302904564317|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2006|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6955.3125|#Level:Host|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8414.0546875|#Level:Host|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:13,209 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:56.1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885113
2022-06-22T15:05:41,191 [INFO ] epollEventLoopGroup-3-5 ACCESS_LOG - /127.0.0.1:59572 "POST /explanation/mnistnet HTTP/1.1" 404 0
2022-06-22T15:05:41,192 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885005
2022-06-22T15:05:51,509 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885151509
2022-06-22T15:05:51,509 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885151509
2022-06-22T15:05:51,510 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend received inference at: 1655885151
2022-06-22T15:05:51,524 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Calculating Explanations
2022-06-22T15:05:51,524 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Getting data and target
2022-06-22T15:05:51,563 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - input shape torch.Size([1, 1, 28, 28])
2022-06-22T15:05:51,563 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.34|#ModelName:mnistnet,Level:Model|#hostname:hello-world-pc,requestID:d9578e2b-48be-4185-9f6a-17eabc2dfec2,timestamp:1655885151
2022-06-22T15:05:51,563 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.37|#ModelName:mnistnet,Level:Model|#hostname:hello-world-pc,requestID:d9578e2b-48be-4185-9f6a-17eabc2dfec2,timestamp:1655885151
2022-06-22T15:05:51,563 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-06-22T15:05:51,563 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-06-22T15:05:51,564 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59574 "POST /explanations/mnistnet HTTP/1.1" 200 55
2022-06-22T15:05:51,564 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885005
2022-06-22T15:05:51,564 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 85560, Backend time ns: 54713803
2022-06-22T15:05:51,564 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 85560, Backend time ns: 54713803
2022-06-22T15:05:51,564 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885151
2022-06-22T15:05:51,564 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885151
2022-06-22T15:06:13,206 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,206 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.01206588745117|#Level:Host|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,207 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.66508102416992|#Level:Host|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,207 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,207 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:16.755186721991702|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,207 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2019|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,207 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,207 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6880.35546875|#Level:Host|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,207 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8491.1328125|#Level:Host|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:13,207 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:56.6|#Level:Host|#hostname:hello-world-pc,timestamp:1655885173
2022-06-22T15:06:46,173 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885206173
2022-06-22T15:06:46,173 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885206173
2022-06-22T15:06:46,173 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend received inference at: 1655885206
2022-06-22T15:06:46,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Calculating Explanations
2022-06-22T15:06:46,189 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Getting data and target
2022-06-22T15:06:46,192 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 19
2022-06-22T15:06:46,191 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - input shape torch.Size([1, 1, 28, 28])
2022-06-22T15:06:46,192 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 19
2022-06-22T15:06:46,192 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:17.53|#ModelName:mnistnet,Level:Model|#hostname:hello-world-pc,requestID:10d6c13c-8aaf-4f43-9e25-5d62a3b52146,timestamp:1655885206
2022-06-22T15:06:46,192 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59576 "POST /explanations/mnistnet HTTP/1.1" 200 32
2022-06-22T15:06:46,192 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885005
2022-06-22T15:06:46,192 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:17.55|#ModelName:mnistnet,Level:Model|#hostname:hello-world-pc,requestID:10d6c13c-8aaf-4f43-9e25-5d62a3b52146,timestamp:1655885206
2022-06-22T15:06:46,192 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 84834, Backend time ns: 19221310
2022-06-22T15:06:46,192 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 84834, Backend time ns: 19221310
2022-06-22T15:06:46,192 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885206
2022-06-22T15:06:46,192 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885206
2022-06-22T15:07:09,399 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885229399
2022-06-22T15:07:09,399 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655885229399
2022-06-22T15:07:09,399 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_LOG - Backend received inference at: 1655885229
2022-06-22T15:07:09,411 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:11.3|#ModelName:mnistnet,Level:Model|#hostname:hello-world-pc,requestID:569b9df9-fafd-4c06-ad4e-3340ef93c72e,timestamp:1655885229
2022-06-22T15:07:09,411 [INFO ] W-9000-mnistnet_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:11.33|#ModelName:mnistnet,Level:Model|#hostname:hello-world-pc,requestID:569b9df9-fafd-4c06-ad4e-3340ef93c72e,timestamp:1655885229
2022-06-22T15:07:09,411 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12
2022-06-22T15:07:09,411 [INFO ] W-9000-mnistnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12
2022-06-22T15:07:09,411 [INFO ] W-9000-mnistnet_1.0 ACCESS_LOG - /127.0.0.1:59578 "POST /predictions/mnistnet HTTP/1.1" 200 20
2022-06-22T15:07:09,411 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885005
2022-06-22T15:07:09,411 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 57618, Backend time ns: 12494848
2022-06-22T15:07:09,411 [DEBUG] W-9000-mnistnet_1.0 org.pytorch.serve.job.Job - Waiting time ns: 57618, Backend time ns: 12494848
2022-06-22T15:07:09,411 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885229
2022-06-22T15:07:09,412 [INFO ] W-9000-mnistnet_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885229
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.01208114624023|#Level:Host|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.66506576538086|#Level:Host|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:16.647302904564317|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2006|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:6|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6825.046875|#Level:Host|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,221 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8545.41015625|#Level:Host|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:07:13,222 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:56.9|#Level:Host|#hostname:hello-world-pc,timestamp:1655885233
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:198.01188278198242|#Level:Host|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:235.66526412963867|#Level:Host|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:54.3|#Level:Host|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:16.713692946058092|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2014|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:23|#Level:Host,device_id:0|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6798.7578125|#Level:Host|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8575.609375|#Level:Host|#hostname:hello-world-pc,timestamp:1655885293
2022-06-22T15:08:13,237 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:57.1|#Level:Host|#hostname:hello-world-pc,timestamp:1655885293
